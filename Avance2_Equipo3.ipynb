{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWxyB4fCOVVDZZ1ITuAk8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Montelongo1992/Avance2-Equipo3/blob/main/Avance2_Equipo3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Construcción de Características.**\n",
        "\n",
        "Cargar y Preprocesar Imágenes: Se cargaron imágenes procesadas de personas autorizadas y no autorizadas, convirtiéndolas a escala de grises y redimensionándolas a 128x128 píxeles. Esto aseguró la homogeneidad en el tamaño de las imágenes para facilitar el procesamiento posterior.\n",
        "Aplanamiento de Imágenes: Las imágenes fueron aplanadas, convirtiendo la representación bidimensional (128x128 píxeles) en vectores unidimensionales para facilitar el uso de algoritmos de aprendizaje automático.\n",
        "\n",
        "**Justificacion**\n",
        "\n",
        "Preprocesamiento aplicado se fundamenta en la necesidad de optimizar los datos para su uso en algoritmos de aprendizaje automático. La conversión de las imágenes a escala de grises elimina la información de color irrelevante, reduciendo la complejidad y posibles ruidos en los datos. El redimensionamiento a 128x128 píxeles asegura que todas las imágenes tengan un formato uniforme, lo que facilita su procesamiento y reduce los costos computacionales. El aplanamiento de las imágenes transforma las matrices bidimensionales en vectores unidimensionales, lo que permite que cada píxel sea tratado como una característica individual por los modelos de clasificación, garantizando así la compatibilidad y eficacia en el análisis de patrones. Estos pasos contribuyen de manera significativa a mejorar el rendimiento del modelo y asegurar su eficiencia durante el entrenamiento."
      ],
      "metadata": {
        "id": "YsPJ6iFG4bG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive para jalar imagenes preprocesadas.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Directorios donde están almacenadas las imágenes de personas autorizadas y no autorizadas ya procesadas\n",
        "carpeta_autorizados = \"/content/drive/MyDrive/data-imagenes/imagenes_procesadas_autorizadas\"\n",
        "carpeta_no_autorizados = \"/content/drive/MyDrive/data-imagenes/imagenes_procesadas_no_autorizadas\"\n",
        "\n",
        "# Función para cargar las imágenes de las carpetas correspondientes\n",
        "def cargar_imagenes_a_matriz(carpeta, etiqueta_clase, tamano_imagen=(128, 128)):\n",
        "    imagenes = []\n",
        "    etiquetas = []\n",
        "\n",
        "    if os.path.exists(carpeta) and len(os.listdir(carpeta)) > 0:\n",
        "        for archivo in os.listdir(carpeta):\n",
        "            img = cv2.imread(os.path.join(carpeta, archivo), cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                # Redimensionar a 128x128 píxeles\n",
        "                img = cv2.resize(img, tamano_imagen)\n",
        "                imagenes.append(img)\n",
        "\n",
        "                # Etiquetamos las imágenes en base al valor pasado por parámetro (1 para autorizados, 0 para no autorizados)\n",
        "                etiquetas.append(etiqueta_clase)\n",
        "        # Convertimos a matrices\n",
        "        imagenes = np.array(imagenes)\n",
        "        etiquetas = np.array(etiquetas)\n",
        "\n",
        "        # Aplanamos las imágenes para tener una representación vectorial\n",
        "        imagenes_aplanadas = imagenes.reshape(imagenes.shape[0], -1)\n",
        "    else:\n",
        "        print(f\"Error: No se encontraron imágenes en {carpeta}\")\n",
        "        imagenes_aplanadas, etiquetas = np.array([]), np.array([])\n",
        "\n",
        "    return imagenes_aplanadas, etiquetas\n",
        "\n",
        "# Cargar imágenes desde las rutas con etiquetas correctas\n",
        "X_autorizados, y_autorizados = cargar_imagenes_a_matriz(carpeta_autorizados, etiqueta_clase=1)\n",
        "X_no_autorizados, y_no_autorizados = cargar_imagenes_a_matriz(carpeta_no_autorizados, etiqueta_clase=0)\n",
        "\n",
        "# Verificar si ambas clases de imágenes fueron cargadas correctamente\n",
        "if X_autorizados.size == 0 or X_no_autorizados.size == 0:\n",
        "    print(\"Error: Asegúrate de que las imágenes estén en las carpetas correctas.\")\n",
        "else:\n",
        "    # Combinar los datos de ambas clases\n",
        "    X = np.concatenate((X_autorizados, X_no_autorizados), axis=0)\n",
        "    y = np.concatenate((y_autorizados, y_no_autorizados), axis=0)\n",
        "\n",
        "    # Mostrar la forma de los datos cargados\n",
        "    print(f\"Forma de las matrices de las imágenes: {X.shape}\")\n",
        "    print(f\"Etiquetas: {y.shape}\")\n",
        "\n",
        "    # Verificar distribución de clases\n",
        "    print(f\"Distribución de clases antes de la división: {Counter(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh5GU3zHkRkF",
        "outputId": "3c6e723f-fd3a-445e-b48d-be2e35cfa0e9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Forma de las matrices de las imágenes: (1144, 16384)\n",
            "Etiquetas: (1144,)\n",
            "Distribución de clases antes de la división: Counter({1: 572, 0: 572})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Normalización**\n",
        "\n",
        "Se utilizó StandardScaler para normalizar los datos de entrada antes de aplicar PCA. Esto asegura que todas las características (valores de píxeles) tengan una media apropiada.\n",
        "\n",
        "**Justificación:**\n",
        "\n",
        "La normalización es esencial porque los modelos de aprendizaje automático, especialmente aquellos basados en distancias como SVM o regresión logística, son sensibles a las escalas de los datos. Al normalizar los datos, garantizamos que las características tengan el mismo impacto en el proceso de entrenamiento."
      ],
      "metadata": {
        "id": "xdrOIZvVkrrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Normalizar los datos\n",
        "escalador = StandardScaler()\n",
        "X_normalizado = escalador.fit_transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "KQqDWDVokpt-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Selección / Extracción de Características.**\n",
        "\n",
        "Se aplicó PCA para reducir la dimensionalidad de los datos, pasando de 16,384 características (píxeles aplanados) a solo 50 componentes principales. Esto mejora la velocidad de entrenamiento y reduce la complejidad del modelo.\n",
        "Justificación cuantitativa: El uso de PCA redujo drásticamente la cantidad de características mientras se mantenía la mayor parte de la información relevante, como se puede ver en la retención de la varianza.\n",
        "\n",
        "**Justificación:**\n",
        "\n",
        "La selección de características con PCA se alineó a los objetivos del análisis de reducir la dimensionalidad sin perder información relevante. Esto facilita la interpretación y reduce el riesgo de sobreajuste, además de mejorar la eficiencia computacional.\n",
        "La reducción de dimensionalidad es clave para evitar un modelo complejo con demasiadas características que puedan inducir ruido en el proceso de clasificación."
      ],
      "metadata": {
        "id": "omlJdLiplDT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar PCA\n",
        "pca = PCA(n_components=50)  # Probar con 50 componentes\n",
        "X_pca = pca.fit_transform(X_normalizado)\n",
        "print(f\"Forma después de PCA con 50 componentes: {X_pca.shape}\")\n",
        "\n",
        "\n",
        "# Mostrar la nueva forma de los datos después de PCA\n",
        "print(f\"Forma después de PCA: {X_pca.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NU4ojDzlDBz",
        "outputId": "022252ec-e75c-4f44-83fd-e33081630606"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma después de PCA con 50 componentes: (1144, 50)\n",
            "Forma después de PCA: (1144, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Conclusiones.**\n",
        "\n",
        "Se incluyó una conclusión clara sobre la preparación de los datos en el contexto de la metodología CRISP-ML. La fase de preparación ha consistido en normalizar, aplicar PCA para extraer características relevantes y ajustar el modelo de manera que la complejidad de los datos se reduzca sin sacrificar precisión.\n",
        "\n",
        "**Conclusión general:**\n",
        "En el contexto de CRISP-ML, la fase de preparación de datos fue realizada cumpliendo con los objetivos de limpieza, reducción de dimensionalidad y selección de características. El conjunto de datos está ahora en una forma óptima para su uso en modelos de aprendizaje automático, lo que nos permite avanzar hacia las fases de modelado y evaluación con confianza en que los datos han sido correctamente procesados."
      ],
      "metadata": {
        "id": "bBRYKK_4l88o"
      }
    }
  ]
}